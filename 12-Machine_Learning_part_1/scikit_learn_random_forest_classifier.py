# -*- coding: utf-8 -*-
"""Random Forest Example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L4iFylrEZI6fDtW5JK2mKQS9SKOE1iDb
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

# Load dataset
data_path = '/content/drive/MyDrive/Colab Notebooks/pima_diabetes_data.csv'
df = pd.read_csv(data_path)

# Inspect dataset
df.head()

# Separating features and target variable
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Split the dataset into training and testing sets
# 70-30 or 80-20 are common ratios
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize our models
# The number of trees in a random forest is to start with 10 times the number of features in your dataset
rf = RandomForestClassifier(n_estimators=80, random_state=42)

# Train Random Forest Classifier on training set
rf.fit(X_train, y_train)

# Predict labels in test test
y_pred_rf = rf.predict(X_test)

# Print accuracy score (one of many ways to assess the performance of a classification model)
# Accuracy is calculated by dividing the number of correct predictions by the total number of predictions
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f'Random Forest Accuracy: {accuracy_rf:.4f}')

# Compute feature importance
importances = rf.feature_importances_
feature_names = X.columns
sorted_idx = np.argsort(importances)
plt.figure(figsize=(8, 6))
plt.barh(feature_names[sorted_idx], importances[sorted_idx])
plt.xlabel('Feature Importance')
plt.title('Feature Importance - Random Forest')
#plt.savefig('rf_feature_importance.png')